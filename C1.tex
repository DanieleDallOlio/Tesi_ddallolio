\chapter{Introduzione}
Questo capitolo introduce i componenti principali del progetto ed ha il compito di spiegarne le operazioni svolte in maniera approfondita.
Tali componenti sono raggruppati nei tre campi su cui è stato condotto il lavoro: la bioinformatica, lo sviluppo informatico e i nodi low power.

La prima sezione, relativa al campo bioinformatico, approfondisce il metodo GATK-LOD\ped{n} riguardante la ricerca sull'origine dei tumori attraverso procedure di natura bioinformatica.
Inoltre, è descritto il ruolo dei fisici in questo ambiente di lavoro e nella creazione degli algoritmi.

La seconda sezione, inerente allo sviluppo informatico, spiega come le funzionalità del programma Snakemake concedano ampie possibilità sulla gestione delle risorse offerte dai nodi delle macchine server.

La terza sezione descrive l'importanza delle macchine low power e specifica quali tipi di nodi sono stati adoperati nelle analisi.


\section{La ricerca delle mutazioni genetiche}
Questa sezione si occupa di dare un'idea generale sugli studi che riguardano le mutazioni genetiche che causano e alimentano i tumori.

Negli ultimi anni l'indagine sulle forme cancerogene basata sulle variazioni che avvengono nel codice genetico ha suscitato sempre più interesse e ciò ha portato allo sviluppo di un discreto numero di programmmi, algoritmi e metodi atti all'analisi del DNA.
Ognuna di queste applicazioni ha come fine la determinazione di quelle mutazioni nei geni che comportano l'insorgere delle malattie tumorali oggi conosciute.
La conoscenza di una tale correlazione è di vitale importanza per la pianificazione di un piano di cura adeguato e, in altri casi, per un intervento anticipato in grado di prevenire il presentarsi della malattia.

Un altro aspetto delicato è la risposta del cancro alle cure a cui è sottoposto il paziente e che, in taluni casi, si concretizza in una forma di resistenza a questi interventi.
La gestione di una tale conseguenza può essere aiutata dalla piena comprensione dell'origine genetica del tumore, la quale agevolerebbe la scelta tra i percorsi di guarigione più opportuni.

Seppur avendo lo stesso obiettivo, gli innumerevoli algoritmi utilizzati spesso si ritrovano in contrasto tra di loro e queste discrepanze sono rilevate quando si procede con il confronto dei risultati finali, che solitamente o non concordano in parte o sono proprio differenti.
Questi algoritmi elaborano i dati sperimentali grezzi attraverso vari processi che generalmente adoperano svariati metodi di statistica, le cui radici provengono dai campi di matematica e fisica applicata.
La netta differenza di prestazioni, insieme alle diverse metodologie operate, lascia ai ricercatori un arduo compito nella scelta del metodo più idoneo da applicare.

In questa tesi il metodo considerato prende il nome di GATK-LOD\ped{n} ed è ideato dalla combinazione di due tra i tools più comuni nel panorama bioinformatico: GATK e MuTect.
Prima di esporre questo algoritmo e le sue fondamenta, è necessario definire il percorso storico che ha determinato le tecniche moderne con cui è analizzato attualmente il materiale genetico e le caratteristiche generali di questi procedimenti.
Inoltre, sarà sottolineato il ruolo degli studi fisici durante il progresso nei campi di biologia e medicina.

\subsection{L'analisi del DNA}
Le ricerche sull'eredità genetica iniziarono dagli studi di Gregory Mendel nella seconda metà del diciannovesimo secolo riguardo all'ibridizzazione tra le piante. 
Nella prima metà del secolo seguente, un serie di contributi, tra cui l'immagine della diffrazione a raggi X della doppia elica da parte di Roselind Franklin, posero le basi alla moderna ricerca genetica.

Il contributo essenziale fu la determinazione della struttura del DNA a cui contribuirono James Watson e Francis Crick, la quale permise di cominciare la decifrazione del codice genetico.
I tentativi di decodifica subirono una svolta significativa nel 
1977 grazie alla tecnica di sequenziamento genetico proposta dal biochimico Frederick Sanger, conosciuta ora come metodo Sanger.

Tale procedimento determina l'ordine delle basi in un filamento breve di DNA utilizzando la tecnica dell'elettroforesi e il supporto di alcuni marcatori radioattivi.
Questo metodo è considerato rivoluzionario ed è stato utilizzato come capostipite del sequenziamento del genoma umano per più di 40 anni.

Nel 1984 il Department of Energy (DOE) e il National Institutes of Health (NIH) degli Stati Uniti avviarono un programma per lo sviluppo di tecnologie e metodi per il sequenziamento genetico e la mappatura del codice genetico umano: lo Human Genome Project(HGP).
L'obiettivo essenziale del programma fu la decodifica del DNA umano che comprese principalemente la determinazione delle sequenze, cioè l'ordine delle basi, e la mappatura dei geni, ovvero la localizzazione dei geni per la maggior parte dei cromosomi e le connessioni con le caratteristiche fisiche e germinali.
Oltre a ciò, il progetto cercò di analizzare anche i genomi di esseri viventi considerati più semplici come ad esempio le mosche.

Molte tecniche per la trattazione del materiale genetico parteciparono alla ricerca, ma il primo procedimento attuato dall'HGP fu il BAC, Bacterial Artificial Chromosome, che sfruttava la clonazione apportata da certi batteri per ottenere un numero elevato di filamenti.
Quest'ultimi subivano un ulteriore frammentazione, per poi essere sequenziati con il metodo Sanger ed infine essere riuniti per formare l'unica stringa di DNA.

Dopo oltre un decennio, nell'Aprile del 2003, l'HGP si concluse con successo, essendo stato decodificato il 99\verb!%! del genoma umano e visto che gli studi furono allargati prematuramente ad altri interessi soprattutto nelle ricerche sulle malattie.

Da quel momento in poi, il termine comune affiliato alle nuove tecniche sul genoma umano è Next-Generation Sequencing(NGS), di cui fanno parte gli algoritmi coinvolti in questa presentazione.

\subsubsection{NGS}
La necessità di garantire una maggior rapidità, e anche di ridurre le spese, ha aperto un altro capitolo nella ricerca sul genoma umano: il Next-Generation Sequecing\cite{Behjati2013}.
Tale necessità deriva dal fatto che il metodo Sanger, per via della precisione necessaria, ha richiesto circa tredici anni per ottenere l'intero genoma umano con un investimento di circa $\$\,2.7$ miliardi.
 
Questa nuova generazione di tecnologie, che sfrutta la parallelizzazione di processi su scale microscopiche \cite{Shendure2008}, concede ai ricercatori strumenti estremamente più veloci, tali da stimare il genoma di un individuo in meno di un giorno, e molto meno costosi.
In aggiunta, i numerosi metodi sviluppati all'interno del NGS sono fortemente radicati sulla teoria dei network. 
Questa branca della fisica e della matematica ha subito nell'ultimo ventennio un notevole progresso e ha acquistato un significato oramai fondamentale nella ricerca medica e biologica. 

Un altro fattore importante che supporta lo sviluppo di tecnologie più veloci è l'esistenza del genoma di riferimento prodotto dal HGP, che permette in certi casi solamente il confronto di frazioni di DNA senza dover estrarre l'intero codice genetico.

Un ulteriore contributo è dato anche dalla contemporanea crescita delle aree che cooperano con la biologia, tra cui l'evoluzione delle tecniche di computazione.

Le tecniche della NGS sono più convenienti del metodo Sanger, dati i tempi richiesti, ma è ancora da trovare il giusto compromesso tra accuratezza dei risultati e diminuzione delle tempistiche e delle spese economiche.
Infatti, nello studio di sequenze a piccola scala la tecnologia Sanger resta tuttora una tra le più affidabili, mentre su letture ad ordini superiori le applicazioni del NGS possono vantare la miglior efficacia.

Pur adoperando diverse tecnologie, gli algoritmi per NGS prevedono solitamente gli stessi passaggi, tra cui l'allineamento delle letture con un riferimento e l'investigazione sulle possibili variazioni.

La difficoltà nello scegliere quale usare tra le diverse opzioni disponibili può essere superata confrontando i risultati finali dei procedimenti, in relazione agli scopi preposti. 
Tali risultati includono la qualità delle informazioni ricavate, le tempistiche previste e altre caratteristiche come l'assemblaggio e gli allineamenti.

Negli ultimi anni si sono aggiunti alle ricerche del NGS due software chiamati GATK e MuTect, i cui metodi hanno condotto allo sviluppo dell'algoritmo coinvolto nel progetto presentato in questa tesi: il GATK-LOD\ped{n}.


\subsection{Il metodo GATK-LOD\ped{n} e le sue radici}
L'ideazione di questo algoritmo é dovuta ad un gruppo di ricercatori del Dipartimento di Fisica e Astronomia dell'Università di Bologna nell'ambito dello studio sulla scoperta di polimorfismi somatici del singolo nucleotide nel sequenziamento dell'esoma.
Precisamente, questo genere di polimorfismo è denonimato con la sigla SNP, single nucleotide polymorphism, e indica quelle variazioni nei singoli nucleotidi che si verificano con frequenza significativa in una specifica posizione del genoma.
In particolare, l'esoma comprende quelle regioni del genoma in cui sono codificate le istruzioni per l'RNA e per la sintesi delle proteine.

Le mutazioni genetiche si distinguono in due tipologie: germinali e somatiche. Le prime identificano le variazioni che avvengono nello zigote e sono condivise in tutte le cellule dell'organismo. Le seconde, invece, sono mutazioni avvenute successivamente allo sviluppo e possono presentarsi a livello di singola cellula o tessuti. 
L'algoritmo GATK-LOD\ped{n} si focalizza sulle mutazioni somatiche perchè esse hanno un ruolo chiave nella progressione della malattia e nella resistenza alla chemioterapia.

L'interesse nel proporre questo metodo nasce dal desiderio di poter predisporre di uno strumento che non si aggiunga al gruppo di software già esistenti, bensì che ottimizzi e potenzi alcuni tra questi.
\'E per questo che il team di studiosi ha considerato due applicazioni standard, GATK e MuTect, e li ha composti in modo da migliorare i prodotti finali di entrambi.
Infatti alla completa esecuzione di GATK è stato applicato una componente di MuTect, ovvero un classificatore Bayesiano conosciuto come LOD\ped{n}, il cui scopo è verificare un ulteriore volta i risultati ottenuti.
I passaggi previsti dall'algoritmo sono vari e di seguito saranno esposti coloro ritenuti più rilevanti.

Dopo aver raccolto i campioni normali e quelli per alcune specie di tumori attraverso specifiche metodologie sperimentali, essi sono stati sottoposti ad un controllo di qualità tale da rimuovere le letture considerate a bassa confidenza.
A questo punto, sono state applicati in successione i tools BWA-MEM e Picard, dove il primo allinea le reads e il secondo le ordina, indicizza e in più ne marca i duplicati.
Una volta completata una nuova fase di riallineamento locale e di ricalibrazione sulla qualità delle letture, grazie all'utilizzo di alcuni strumenti del Genome Analysis Toolkit, sono stati eseguiti GATK e MuTect per la ricerca delle varianti sui singoli nucleotidi(SNV).

La differenza procedurale tra queste due applicazioni è che, mentre MuTect ritrova le mutazioni contemporaneamente tra i campioni normali e tumorali, GATK le chiama indipendentemente.
Si sottolinea che i due metodi scovano varianti che non sono condivise da entrambi, indicando la natura incompleta dei sistemi utilizzati.
Un ulteriore distinzione tra GATK e MuTect è data dal tipo di risultati raccolti poichè, se il primo è dotato di una maggiore sensibilità alle mutazioni, dalle 3 alle 20 volte superiore al secondo, quest'ultimo possiede una maggiore specificità degli SNVs.
Avendo GATK un elevato numero di falsi positivi nella chiamata alle varianti, è stato aggiunto in fondo all'algoritmo il classificatore Bayesiano di MuTect per cercare di ridurre questo errore.
Il compito del LOD\ped{n} consiste nel calcolare il rapporto tra i due seguenti eventi probabilistici.
Il primo è il caso in cui le mutazioni nel campione normale siano dovute a rumori di fondo e quindi in realtà non esistano.
Il secondo, invece, considera il caso in cui la mutazione esista davvero nel campione normale e sia dovuta ad una variante germinale eterozigote.
A questo punto, se il rapporto tra le probabilità (il Log Odds, da cui la sigla LOD) eccede un valore di soglia fissato, il classificatore definisce la variante come somatica.

Nella ricerca sono stati confrontati i prodotti finali dei tre algoritmi ed è stato verificato che l'uso di GATK-LOD\ped{n} riduce notevolmente il numero di chiamate degli SNVs di GATK, mantenendo una sensibilità nettamente superiore a MuTect.
Questa riduzione è dovuta all'eliminazione di un sostanziale numero di falsi positivi, la cui entità dipende dalla tipologia di tumore esaminato.

Il miglioramento dei precedenti metodi, però, non si limita solo alla filtrazione dei falsi positivi ma anche al mantenimento della sensibilità di GATK e ad un aumento della specificità.
Quest'ultimo è indicato dal fatto che GATK-LOD\ped{n} abbia presentato frequenze di validità superiori e un miglior PPV(Positive Predictive Value) rispetto a GATK su vari numeri di reads di VAF(Variant Allelic Frequency).
Le metodologie adoperate per confrontare la sensibilità e la specificità non sono state ritenuti utili allo scopo della presentazione e per questo non verranno trattate.

A posteriori delle indagini sperimentali, GATK-LOD\ped{n} si è rivelato uno strumento utile ad allargare le capacità di GATK e a individuare varianti non trovate da MuTect senza dover rinunciare alla specificità e sensibilità.

Visti i risultati positivi ricavati dall'algoritmo, l'articolo originale sostiene che un metodo di questo tipo possa aiutare a definire con maggior dettaglio le mutazioni somatiche di genomi cancerogeni, favorendo le valutazioni mediche e gli approcci sui percorsi di cura.

Per fornire una conoscenza più approndita delle operazioni che avvengono nel sequenziamento, saranno esposti %brevemente il meccanismo statistico e 
i passaggi fondamentali alla base delle applicazioni utilizzate nel metodo GATK-LOD\ped{n}: GATK e MuTect.
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{GATK-LODn.png}
\caption{Pipeline di GATK-LOD\ped{n}(presa da \cite{DoValle2016}).}
\label{fig: GATK-LODn}
\end{figure}

%\subsection{La statistica di base}

\subsubsection{GATK}
Il Genome Analysis Toolkit è un framework di programmazione creato da un gruppo di ricercatori di Boston, Massacchussets, assieme al Broad Institute di Harvard e l'MIT in Cambridge, Massacchussets, per facilitare lo sviluppo di programmi che analizzano l'enorme mole generati dal NGS, Next Generation DNA Sequencing \cite{McKenna2010}.
Infatti, l'utilizzo di questo sistema permette di sviluppare tools più solidi e performanti per il sequenziamento genetico.

La mancanza di strumenti flessibili e sofisticati dediti alla manipolazione dei dati di sequenziamento in maniera programmatica ha portato alla creazione di GATK.
Infatti, la maggior parte dei software che supportano l'analisi del DNA concedono alte prestazioni solo nella specifica area di interesse, senza mantenerle su differenti ambiti.
Questo deficit e l'emergere di un formato specifico per i prodotti del sequenziamento (SAM), hanno dato l'opportunità di ideare un software per la semplificazione delle analisi sui set di dati.

L'architettura di base per GATK è il MapReduce, il cui funzionamento implica la separazione delle computazioni in due passaggi.
Nel primo, l'intero problema è suddiviso in tanti elementi discreti indipendenti, i quali sono correlati alla funzione Map; nel secondo step, l'operatore Reduce riunisce gli esiti di Map in un unico risultato finale.
Siccome solo in certi casi, come la ricerca degli SNPs, vi è un adattamento naturale del sistema, GATK è costruito su traversals e walkers.
Le traversals sono schemi che provvedono alla preparazione e divisione dei dati; mentre le walkers consistono nei differenti moduli di analisi che computano i dati provenienti dalle prime.
Anche se GATK ha un numero ridotto di traversals, questo basta per soddisfare le esigenze della maggior parte della comunità di ricerca.
I due trasversal standard sono il "by each sequencer read" e il "by every read covering single base position in a genome", il cui uso è sfruttato per operazioni standard come la chiamata agli SNPs.
Il meccanismo di questi schemi non è riportato in quanto non rilevante per il lavoro di tesi.

Uno dei punti di forza dell'algoritmo è la capacità di gestire l'enorme quantità di materiale ricavato dal sequenziamento.
In particolare, GATK divide il tutto in pezzi chiamati "shard" che, al contrario della maggior parte dei sistemi di suddivisione, sono di una dimensione di poche migliaia di basi.
In questo modo non sono limitate le capacità della memoria e le prestazioni nel caso di parallelismo.
Questi shards contengono tutte le informazioni della regione genomica associata e sono trasmesse al trasversal prescelto.

Altre caratteristiche di GATK sono la possibilità di selezionare solo certe regioni del genoma, la parallelizzazione delle azioni da svolgere, l'organizzazione dei files di input grazie ad un'operazione di merging e la presenza di un walker relativo alla depth of coverage(DoC).

Il metodo stima il genotipo più probabile attraverso un semplice algoritmo Bayesiano, che ha funzione sia di punto di partenza per sviluppare nuovi classificatori; che di mettere in luce le capacità di parallelizzazione e di ottimizzazione della memoria disposte da GATK.
\'E importante sottolineare che è proprio la semplicità dell'operatore la causa di molti falsi positivi chiamati.

Il Genome Analysis Toolkit è quindi un framework che, grazie al suo nuovo approccio ai big data e alla piena libertà di sviluppo, fornisce strumenti importanti per l'elaborazione di algoritmi più specifici come il GATK-LOD\ped{n}.

\subsubsection{MuTect}
Il metodo MuTect è un algoritmo per la rilevazione delle mutazioni genomiche, che è stato creato per superare le scarse prestazioni dei meccanismi fino ad allora presenti \cite{Cibulskis2013}.
Infatti, quest'ultimi fornivano livelli di sensibilità e specificità considerati insoddisfacenti per una sufficiente comprensione delle anomalie.

Il sistema si focalizza soprattutto sull'individuazione delle varianti a bassa frazione allelica, ovvero quelle regioni del DNA che originano e nutrono il tumore.
Queste frazioni sono tanto importanti quanto difficili da scovare poichè, mentre il meccanismo al loro interno determina il tipo di alterazione genomica, sia le regioni in cui si manifestano che la frequenza di occorrenza sono basse.

La conoscenza di questi tratti specifici favorisce lo studio sulle evoluzioni delle forme cancerogene e aiuta a proporre terapie di cura più affidabili.

L'esistenza di metodi a scarsa sensibilità e specificità ha quindi indotto un team di ricercatori dell'università di Boston, Massachussetts, assieme al Broad Insitute di Harvard e all'MIT in Cambridge, Massacchussets, a sviluppare il tool MuTect.
Questo strumento si è dimostrato sensibile e specifico nella scoperta degli eventi a bassa frequenza allelica, mantenendo alte prestazioni di specificità anche a frequenza superiori.

Nessuno dei modelli precedenti supporta tutti gli errori dei processi di sequenziamento ed è per questo che MuTect sfrutta due approcci di benchmarking per migliorare la performance: il downsampling e  i 'virtual tumors'.
Il primo misura la sensibilità con cui le mutazioni vengono chiamate, grazie a subset di dati con mutazioni già riconosciute.
Questo approccio è però limitato da alcuni aspetti che comprendono: il basso numero di eventi verificati, la sovrastima della sensibilità e l'impossibilità alla misurazione della specificità.
A causa dei limiti del downsampling è presente anche un secondo approccio, 'virtual tumors', che genera dei tumori virtuali conosciuti in ogni dettaglio.
I questa maniera i due metodi sono complementari e mentre il primo usa dati reali ma è limitato, il secondo è libero ma consuma materiale virtuale generato.
La sintesi dei due approcci permette di ricavare valori più veritieri della  sensibilità, misurare la specificità e colmare la maggior parte delle lacune derivate dal downsampling.

Previo allineamento ed esecuzione dei processi standard preanalisi, MuTect riceve i dati sequenziati sia dei campioni normali che dei cancerogeni, per poi eseguire quattro operazioni principali: la rimozione di dati a bassa qualità, la ricerca delle varianti, il filtraggio dei falsi positivi e la classificazione delle varianti.
La ricerca delle varianti consiste nell'applicazione di un primo metodo bayesiano, detto LOD\ped{T}, che adopera il rapporto tra due eventi probabilistici per determinare se è presente un variante.
Siccome il calcolo è condizionato da errori di sequenziamento, prima di ricavare la probabilità del variante è necessario applicare una serie di filtri che ne elimini la maggior parte, evitando così la sottostima dei falsi positivi.
Successivamente, si utilizza il secondo classificatore bayesiano LOD\ped{n}, implementato nel metodo GATK-LOD\ped{n}, per definire se il variante è somatico, germinale o indeterminato.

Gli esperimenti di verifica sulla sensibilità hanno evidenziato come MuTect sia uno strumento ad alto rilevamento soprattutto nelle mutazioni a frazioni alleliche basse.
Riguardo alla specificità sono due le fonti principali di falsi positivi: l'eccessiva chiamata a varianti, dovuta ad errori di sequenziamento, e la scarsa individuazione di eventi germinali, causato dalle insufficienti letture sul campione normale.
La gestione di tali errori è risolta con il miglior compromesso verificato tra sensibilità e specificità, che risulta nella scelta di mantenere alta la seconda a discapito della prima.

Complessivamente MuTect è un metodo che valorizza il compromesso tra il grado di rilevazione delle varianti e la loro corretta classificazione, producendo risultati affidabili.
In aggiunta, riporta sostanziali miglioramenti sulle analisi delle mutazioni a bassa frequenza allelica, la cui importanza è essenziale per le future ricerche biomediche.

\subsection{Il ruolo della fisica nella ricerca biomedica}
Nella prima metà del Novecento, dopo che l'introduzione della relativià di Einstein e la nascita della meccanica quantistica avevano ribaltato il pensiero della comunità scientifica, numerosi fisici si interessarono a problemi di biologia.
Il contributo fornito ad una così diversa area era sia di tipo matematico, dove la necessità di utilizzare equazioni e formule continuava ad aumentare nel corso degli anni, che di tipo teorica, ove l'uso di modelli già presenti nella fisica erano utili ad una comprensione più approfondita dei quesiti di biologia.
Il libro "What is life?" del fisico austriaco Erwin Schroedinger promosse il ruolo della fisica negli studi sull'ereditarietà a tal punto che gli scienziati Watson e Crick, che svelarono la struttura del DNA, lo accreditarono nelle loro pubblicazioni \cite{2058-7058-12-9-22}.
In seguito, numerosi fisici hanno contributo alla nascita della biologia molecolare e questa branca ha continuato a beneficiare della fisica sia per interpretare i problemi sotto un'ottica diversa, che per la confidenza che i fisici hanno riguardo alla modellizzazione dei sistemi complessi.
Difatti, la richezza della fisica nel saper gestire tali sistemi, l'elasticità nell'ampliare le proprie conoscenze a differenti campi di studio, insieme ad un approccio critico ma propositivo, si è rivelata un ingriedente necessario per il progresso della materia \cite{Pooley2005}.

Al passare del tempo e con il crescere delle ricerche, soprattutto nel campo biomedico, la presenza della fisica ha acquistato importanza sia nel ramo sperimentale che in quello teorico.
Nel primo, difatti, per anni le tecniche della cristallografia e della risonanza magnetica sono stati essenziali per i biologi molecolari, e, tuttora, le innovative tecniche della NGS concentrano gli sforzi su scovare quelle differenti proprietà fisiche che permettono di stabilire più velocemente i nucleotidi \cite{Zwolak2008}.
Nel secondo, invece, la fisica coopera con i bioinformatici sia nell'individuazione di modelli efficaci con cui affrontare i big data provenienti dai laboratori, che per l'analisi delle sequenze di DNA, sfruttando metodi provenienti esattamente dalla fisica statistica.

Nello specifico, la maggioranza dei modelli utilizzati e studiati sono basati sulla teoria dei network, la quale ha acquisito uno spessore essenziale negli studi di biologia e di medicina.

\subsubsection{Teoria dei network}
Per network si intende un modello che mira a rappresentare un sistema complesso, cercando di modellizzare le caratteristiche collettive. 
Un network è composto da elementi chiamati nodi, i quali sono collegati tramite percorsi, detti path, e tra cui sono presenti relazioni, denominate link. 
La teoria dei network è stata adottata e sviluppata per spiegare quelle leggi della natura di carattere generico e che hanno origine da manifestazioni stocastiche.
Gli strumenti matematici adoperati in questa teoria sono prevalentemente le matrici e i grafi, le cui proprietà garantiscono la massima espressione di connettività e di evoluzione che contraddinguono i sistemi complessi. 

I sistemi presenti in biologia sono generalmente un insieme di oggetti fortemente interagenti di cui si conoscono le proprietà; come possono essere ad esempio le reti geniche o il sistema nervoso umano.
Grazie alle tecniche provenienti dalla teoria dei network non è più indispensabile ricostruire il codice genetico puntualmente, dato che risulta possibile ricombinare tante stringhe casuali dello stesso filamento di DNA(shotgun sequencing).
Questa novità è ciò che contraddistingue le nuove tecniche del NGS dal metodo Sanger e che permette a queste di ricavare i genomi di un individuo in tempi decisamente più ristretti.

La ricostruzione del genoma avviene, in particolare, a partire da tutte le possibili sovrapposizioni dei frammenti sperimentali estratti. 
La scelta riguardo al ruolo dei frammenti e delle sovrapposizioni, ovvero stabilire chi è nodo e chi è link, è ciò che determina il modello dei network da perseguire.
Nel caso in cui i primi fossero i nodi e le seconde i link condurrebbe all'utilizzo di un path Hamiltoniano, cioè un percorso dove tutti i nodi vengono usati una volta sola.
All'opposto si avrebbe un path Euleriano, dove ogni link deve essere processato una singola volta e dove il problema è identificato dal grafo di De Brujin.
La differenza più netta tra i due path è la crescita di complessità, visto che il primo ha andamento non polinomiale mentre il secondo lineare, ed è da ciò che si valutano le capacità computazionali.

In conclusione, la teoria dei network, unita all'utilizzo di macchine server performanti, ha accelerato lo sviluppo di meccanismi sempre più raffinati per la ricostruzione del DNA, superando l'esaminazione prolungata in laboratorio con l'applicazione di congetture di fisica statistica.    

\section{Lo strumento di sviluppo: Snakemake}
\label{sec:SM}
Snakemake è un sistema di gestione dei flussi di lavoro che semplifica l'esecuzione di algoritmi particolarmente complessi grazie all'utilizzo di un ambiente di sviluppo nitido ed intuitivo \cite{Koster2012}.
In più, questo software è specializzato nella scalabilità dei lavori, da singoli core fino all'uso di cluster, le cui transizioni non implicano pesanti modifiche al procedimento del sistema.

Questo programma è basato sul linguaggio di programmazione Python \cite{Python} e la sua formazione è fortemente influenzata dal noto tool Make del sistema operativo Linux.
Ciò significa che Snakemake è modellato su una strategia di tipo \textit{pull} proprio come Make.
In un procedimento che segue la strategia \textit{pull}, ogni lavoro coinvolto specifica ciò di cui ha bisogno per essere eseguito ed il programma esegue automaticamente altri lavoro che posso soddisfare queste richieste; la sequenza di esecuzione è quindi determinata a partire dal lavoro finale.
Al contrario, in un procedimento che segua la strategia \textit{push} ogni lavoro determina quale sarà il lavoro eseguito successivamente.
La strategia \textit{pull} richiede una maggior pianificazione per la scrittura ma fornisce diversi vantaggi fra cui una maggior semplicità di parallelizzazione e la possibilità di riprendere l'esecuzione del procedimento dopo un fallimento critico senza dover ripetere tutti i passi precedenti.

Il contenitore del codice Python in Snakemake è chiamato di default \textit{Snakefile} e l'ordine di esecuzione predefinito da linea di comando è:
\begin{lstlisting}
$ snakemake
\end{lstlisting}
Il sistema è strutturato, come Make, da un insieme di regole che rappresentano i compiti da svolgere nell'algoritmo, dove ognuna di esse contiene le tre informazioni fondamentali: input, output e azione.
\begin{lstlisting}[language=Python]
rule 'nome':
	input:

	output:

	shell/run/script:

\end{lstlisting}
Come si può vedere dal codice riportato, l'operazione avviene etichettando la regola con un certo 'nome' e inserendone all'interno le keyword \textit{input}, \textit{output} ed una tra \textit{shell}, \textit{run} e \textit{script}.
Rispettivamente le tre chiavi implicano l'utilizzo di comandi da terminale, l'esecuzione di un codice Python e l'avvio di uno script esterno.
La dichiarazione dei file di input e di output esprime le condizioni iniziali per la regola e il risultato atteso, così permettendo al programma di riconoscere le relative dipendenze e stabilire l'ordine di successione dei singoli lavori.
Per facilitare la comprensione del codice, è possibile creare, grazie al comando \textit{dot} della libreria Graphviz, un diagramma che schematizzi la sequenza delle regole.
Infatti questa sequenza, che ha il nome di \textit{DAG} (Directed Acyclic Graph), è visualizzata da \textit{dot} in una struttura ramificata, in cui i lavori sono rappresentati da nodi e le dipendenze sono semplicemente descritte da linee congiungenti. 
Questa schematizzazione consente anche di identificare quali dei lavori sono parallelizzabili e quali, invece, devono mantenere una prestabilita sequenzialità. 
Ciò è possibile proprio dal fatto che il comando \textit{dot} rappresenta la successione delle regole solo e unicamente in base alle dipendenze reciproche, mantenendo quindi, ad esempio, la ripetizione di alcune regole sullo stesso livello di operatività.
Questo sistema di riconoscimento dei lavori potenzialmente simultanei da un contributo importante per impostare meccanismi di parallelizzazione. 

A proposito della gestione tecnica dei lavori, Snakemake ha un certo numero di proprietà che consentono di selezionare le caratteristiche computazionali desiderate. 
Due tra le funzionalità più rilevanti sono \textit{resources} e \textit{cores} che stabiliscono, da linea di comando, rispettivamente quali risorse delle macchine sono a disposizione e quanti core sono fruibili.
Le risorse possono includere ad esempio delle direzioni sulla gpu o sulla memoria accessibile (opzione \textit{mem}), mentre il numero di cores è essenziale per gestire i threads.

I threads sono fondamentali per un'esecuzione simultanea e il loro utilizzo avviene inserendo l'attributo \textit{threads} nel dominio di una regola.
L'assegnazione di un determinato numero di threads è comunque influenzato da un'eventuale opzione sui cores, dato che il numero di threads non può eccedere il numero di cores utilizzabili.
In particolare, oltre ai threads, è possibile specificare singolarmente per ogni regola anche le risorse disponibili, aggiungendo la voce \textit{resources} nel dominio. 

Altre due proprietà di Snakemake sono la portabilità e un innovativo meccanismo di inferenza.
La prima manifesta le poche dipendenze di installazione, dato che è in generale sufficiente dotarsi di Python; mentre la seconda rappresenta un moderno supporto all'inferenza per nome che si compie grazie a wildcards nominate nelle regole.

Le wildcards consistono in nomi che agiscono come parametri e che servono ad automatizzare le operazioni di riconoscimento delle dipendenze tra regole. 
In dettaglio, quando una variabile è associata ad una wildcard, è naturale che questa sia associata a più valori e che, quindi, il nome della variabile sia semplicemente una chiave. 
In presenza di una wildcard nell'input di una regola, i valori in essa contenuti sono ricercati negli output delle altre regole e, dopo aver tracciato le dipendenze, tale regola è eseguita una volta per ogni valore attribuito alla chiave.
Così agendo, le ripetizioni delle regole su diversi valori della wildcard sono sullo stesso piano di esecuzione, favorendo la parallelizzazione dei lavori.   
  

Nel corso di questa tesi sono stati implementati alla pipeline altri attributi forniti da Snakemake.
Infatti, eccetto le chiavi di base, esistono diverse funzionalità che arrischiscono l'impostazione e la realizzazione delle regole, tra cui \textit{params}, \textit{benchmark} e \textit{conda}.
Il primo indica i parametri indispensabili introdotti nella regola mentre \textit{benchmark} e \textit{conda} richiedono una trattatazione più ampia.

In seguito alla spiegazioni di questi, è necessario dedicare un breve accenno ai file di configurazione spesso affiancati agli Snakefile.

\subsubsection{benchmark}
\label{subsubsec: SMb}
Quando è utilizzata la direttiva \textit{benchmark} in una regola, Snakemake trascrive su un file di testo i dettagli tecnici dell'operazione svolta.

Per primi sono riportati il tempo impiegato dal nodo per completare la regola sia in secondi che in ore, minuti e secondi.

A seguire, dal terzo al sesto sono mostrate le informazioni sull'uso della memoria.
In particolare:
\begin{itemize}
\item max\verb!_!rss è la massima memoria fisica \textit{non swapped}, che il processo usa(Resident Set Size);
\item max\verb!_!vms è la massima quantità totale di memoria virtuale utilizzata(Virtual Memory Size);
\item max\verb!_!uss è il massimo di memoria affidata unicamente al singolo lavoro, che esso impiega(Unique Set Size);
\item max\verb!_!pss è il massimo della quantità condivisa tra tutti i processi, che la regola sfrutta(Proportional Set Size).
\end{itemize}
Riguardo agli ultimi tre dettagli, sono presenti io\verb!_!in e io\verb!_!out che identificano le caratteristiche di input e output del processo; e mean\verb!_!load che descrive il carico medio sulla CPU.

\subsubsection{Conda}
Conda è una piattaforma per la gestione di svariati pacchetti ed è un pratico amministratore degli ambienti di elaborazione.
La cooperazione tra Snakemake e Conda, che avviene, come mostrato nel codice sottostante, inserendo la direttiva \textit{conda} nel dominio, favorisce un uso più flessibile degli ambienti.
\begin{lstlisting}
	conda:
		"path/to/directory/config_file.yaml"
\end{lstlisting}
Difatti, prima che lo Snakefile sia eseguito, Snakemake riconosce la voce \textit{conda} nella regola e richiama Conda per la creazione o attivazione dell'ambiente su cui essa sarà completata.
Le informazioni sull'ambiente da formare, nel caso esso debba essere creato, sono procurate da un file di configurazione indicato nel dominio ed è proprio questo che consente a Conda di generare l'ambiente e dotarlo dei requisiti richiesti.
Una volta che Conda ha terminato la creazione, l'ambiente è attivato e comincia l'esecuzione.
Chiaramente, questa fase di creazione avviene alla prima richiesta di un ambiente con determinate caratteristiche, così da procedere semplicemente con l'attivazione nelle successive computazioni.

Una tale opzione non costringe altri utilizzatori ad impostare manualmente l'ambiente principale come voluto dallo Snakefile, dato che ne sarà creato un apposito per la regola, e ciò ne alleggerisce l'utilizzo.
In aggiunta, questo meccanismo rende plastica la realizzazione del codice, vista la possibilità di dotare ogni regola di un proprio ambiente.


\subsection{I file di configurazione in Snakemake}
I file di configurazione sono oggetti, solitamente in formato \textit{yaml}, dedicati alle istruzioni sui parametri contenuti nei codici principali.
Il ruolo di tali file è stabilire il valore delle chiavi che rappresentano i parametri.
Ad esempio, nel caso di Snakemake, un tipico file di configurazione ha forma: \verb! chiave: 'valore'!.
Il valore può essere un numero, una parola, un percorso o un file, ed esso rimane costante se non modificato direttamente nel codice sorgente o da linea di comando.
 Nell'ambito di Snakemake, il file di configurazione deve essere citato inizialmente con la linea \verb!configfile:'config_file.yaml'! e l'associazione tra chiavi e parametri è conseguita inizializzando una variabile secondo la seguente modalità.
\begin{lstlisting}[language=Python]
parametro = configfile['chiave']
\end{lstlisting}
I valori dei parametri possono essere modificati da terminale nel comando di avvio dello Snakefile, grazie all'argomento \textit{--config} seguito da una nuova inizializzazione, ad esempio \verb!chiave='nuovo_valore'!.

Il file di configurazione che si indica nella direttiva \textit{conda} ha, invece, una composizione differente, come mostrato nel codice riportato.
\begin{lstlisting}[language=Python]
channels:
  - esempio_canale
dependencies:
 -  esempio_dipendenza
\end{lstlisting}
Il dominio \textit{channels} determina su quale canale Conda deve lavorare, mentre \textit{dependencies} specifica quali pacchetti devono essere installati nell'ambiente.
In questo modo, dopo che Conda è chiamato da Snakemake per l'attivazione del particolare ambiente, esso controlla se ne è già presente uno con tali proprietà e, se esistente, lo attiva o, al contrario, prima lo istanzia.

Il contenuto dei file di configurazione è, in conclusione, determinante per il corretto, oltre che miglior, funzionamento del sistema.

\section{Le macchine low power e i nodi utilizzati}
\label{sec:LP}

I gruppi di ricerca impegnati per l'analisi dei big data in campo biomedico, hanno a disposizione server sempre più sofisticati e veloci. Purtroppo le migliori prestazioni e funzionalità hanno comportato costi e consumi elettrici sempre piu' elevati (anche se negli ultimi anni sono state introdotte anche per i server tecnologie avanzate per il power-saving e il power-monitoring). 
D'altra parte i processori low-power normalmente utilizzati in ambito low-end o embedded/mobile stanno acquisendo un ruolo sempre piu' importante.

Si è quindi creato nel mondo computazionale scientifico un inedito scenario: accanto ai tradizionali server high-end, costosi ed energivori, si iniziano a intravedere soluzioni ibride con processori low-power o acceleratori che stanno progressivamente riducendo il divario prestazionale rispetto ai server high-end. 

I laboratori di calcolo e i data center scientifici e commerciali seguono con interesse l'evoluzione delle tecnologie di calcolo low-power per un duplice motivo: costo elevato delle macchine server attuali e costi dell'energia elettrica per il consumo dei server e raffreddamento. Il costo medio per macchine server high-end raggiungono ormai cifre molto elevate, dell'ordine delle migliaia di euro. I costi energetici per il mantenimento in servizio dei server high-end e quello per il raffreddamento hanno raggiunto ormai livelli proibitivi. Un esempio è il data center di medie dimensioni a Bologna del INFN-CNAF (Centro Nazionale delle Tecnologie Informatiche dell'INFN) dove sono ospitati migliaia di server il cui costo annuale d'esercizio per l'energia elettrica è dell'ordine delle centinaia di migliaia di euro.

Un altro fattore penalizzante è dato dalla mancanza di scalabilità e flessibilità per aggiornare l'hardware dei server, visto il notevole investimento per l'acquisto giustificato da un utilizzo almeno triennale dell'hardware prima di essere sostituito. D'altro canto schedine low-power, grazie a costi particolarmente contenuti, consentono una notevole flessibilità nell'acquisto di nuovo hardware non appena viene rilasciato dal produttore.

La tecnologia di calcolo low-power utilizza prevalentemente Systems-on-Chip (SoCs) spesso di derivazione embedded o mobile che sono stati disegnati per garantire le massime prestazioni computazionali coi minimi consumi elettrici. L'ottimo rapporto prestazioni/consumi dei SoC è dovuto all'impegno dei costruttori per soddisfare la domanda di schede e dispositivi sempre più sensibili alle performance e alla riduzione dei consumi elettrici da parte dell'emergente industria mobile ed embedded. 
Perciò, visti i notevoli miglioramenti degli ultimi anni, i SoC stanno seriamente diventando un'interessante alternativa per le applicazioni scientifiche senza sacrificare troppo performance e funzionalità dei server tradizionali. Inoltre la cooperazione fra diversi nodi low-power, configurati in cluster, potrebbe ambire confrontarsi con i cluster tradizionali di server high-end.

Questa tesi descrive una ricerca introduttiva per valutare prestazioni e funzionalità su hardware low-power di un'applicazione scientifica che gira normalmente su macchine server.

Le macchine low-power coinvolte in questa tesi sono state scelte con caratteristiche tecniche differenti pur mantenendo la stessa architettura X86\_64. In più, è stato considerato anche un server tradizionale X86\_64 per poter comparare le diverse esecuzioni.

Le macchine low-power sono state scelte, installate e configurate grazie alla collaborazione con il CNAF (Centro Nazionale per la Ricerca e lo Sviluppo sulle Tecnologie per l'Informazione e la Comunicazione) dell'Istituto Nazionale di Fisica Nucleare.

\pagebreak
La figura \ref{fig:Cluster} mostra le macchine utilizzate per i test.


\begin{figure}[H]
\centering
\subfloat[][\emph{Cluster Xeon D-1540 e Atom C2750.}]
	{\label{subfig:low_power}
	\includegraphics[scale=.5]{Avoton_Xeond.jpg}
	} \quad
\subfloat[][\emph{Cluster contenente bio8.}]
	{\label{subfig:bio8}
	\includegraphics[width=.46\textwidth]{bio8.png}
	} \\
\caption{}
\label{fig:Cluster}
\end{figure}

I dettagli sui nodi dei cluster utilizzati per le computazioni sono esposti nelle tabelle sottostanti \ref{tab:cluster_generali} e \ref{tab:cpu}, che includono sia i tre nodi low power che un nodo di un server tradizionale (bio8).
In particolare, tutti i nodi montano processori con architettura X86\_64 su sistema operativo Linux Debian 9.

\begin{table}[H]
\begin{threeparttable}
\resizebox{1.0\textwidth}{!}{%
$\begin{array}{*{6}{c}}
	\toprule
		Nodo & CPU & Memory & Storage & Costo\text{*} & Consumo\text{*}  \\
	\midrule
		xeond & \text{1x Xeon D-1540} & 16\,GB & 8\,TB(HDD) & \text{\euro 1000} & 60\,W\\
		avoton & \text{1x Atom C2750}  & 16\,GB & 5\,TB(HDD) & \text{\euro 600} & 30\,W\\
		n3700 & \text{1x Pentium N3700}  & 8\,GB & 0.5\,TB(SSD) & \text{\euro 130} & 8\,W \\
	\midrule
		bio8 & \text{2x Xeon E5-2620v4} & 128\,GB & 2\,TB(HDD) & \text{\euro 10000} & 180\,W\\		
	\bottomrule
\end{array}$%
}
\begin{tablenotes}\footnotesize
\item[*] I valori di costo e consumo energetico sono stimati.
\end{tablenotes}
\end{threeparttable}
\caption{Caratteristiche dei nodi.}
\label{tab:cluster_generali}
\end{table}

\begin{table}[H]
\centering
\resizebox{1.0\textwidth}{!}{%
$\begin{array}{*{6}{c}}
	\toprule
		CPU & Microarchitecture(Platform)/litho & Freq(GHz) & Cores & Cache & TDP\\
	\midrule
		\text{Xeon D-1540} & Broadwell/14nm & 2.0(2.60) & 8(16) & 12\,MB & 45\,W \\
		\text{Atom C2750} & Silvermont(Avoton)/22nm & 2.40(2.60) & 8 & 4\,MB & 25\,W \\
		\text{Pentium N3700} & Airmont(Braswell)/14nm & 1.60(2.40) & 4 & 2\,MB & 6\,W \\
	\midrule
		\text{Xeon E5-2620v4} & Broadwell-EP/14nm & 2.10(3.00) & 8(16) & 20\,MB & 85\,W \\
	\bottomrule
\end{array}$%
}
\caption{Caratteristiche delle CPU.}
\label{tab:cpu}
\end{table}

Si sottolinea che, in realtà, il nodo xeond ha una cpu con TDP pari a 45\,W e quindi è considerato al limite come macchina low power.
Inoltre il tipo di processore è classificato come tipologia server, esattamente come quello di bio8.

